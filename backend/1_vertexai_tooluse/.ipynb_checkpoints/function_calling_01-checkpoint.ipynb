{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9416a7e0-5daa-46e0-9278-44d69ea2c534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bc76b8-b7b5-44b1-9e69-b55bf4a7b690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a98fc54-7a51-4e0b-9c89-97bd1116038c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"angelic-bee-193823\"  \n",
    "LOCATION = \"us-central1\"  \n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2304e69c-05e2-42f8-b639-edc1f3987296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[compute]\n",
      "region = us-central1\n",
      "[core]\n",
      "account = marvinngesa@gmail.com\n",
      "disable_usage_reporting = True\n",
      "project = angelic-bee-193823\n",
      "[dataproc]\n",
      "region = us-central1\n",
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102f1505-5f7c-4718-aca4-05bc3e1bf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from vertexai.generative_models import (\n",
    "    FunctionDeclaration,\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80e966-1a17-482c-a10e-ceebea0d360f",
   "metadata": {},
   "source": [
    "#### Chat example: Using Function Calling in a chat session to answer user's questions about the Google Store\n",
    "\n",
    "In this example, you'll use Function Calling along with the chat modality in the Gemini model to help customers get information about products in the Google Store.\n",
    "\n",
    "You'll start by defining three functions: one to get product information, another to get the location of the closest stores, and one more to place an order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91a2549-9e6e-49ce-a07c-4fc85e827041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_product_info = FunctionDeclaration(\n",
    "    name=\"get_product_info\",\n",
    "    description=\"Get the stock amount and identifier for a given product\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"product_name\": {\"type\": \"string\", \"description\": \"Product name\"}\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "get_store_location = FunctionDeclaration(\n",
    "    name=\"get_store_location\",\n",
    "    description=\"Get the location of the closest store\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"Location\"}},\n",
    "    },\n",
    ")\n",
    "\n",
    "place_order = FunctionDeclaration(\n",
    "    name=\"place_order\",\n",
    "    description=\"Place an order\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"product\": {\"type\": \"string\", \"description\": \"Product name\"},\n",
    "            \"address\": {\"type\": \"string\", \"description\": \"Shipping address\"},\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4758b-0010-4f99-a4c3-a2182f8898cb",
   "metadata": {},
   "source": [
    "Note that function parameters are specified as a Python dictionary in accordance with the OpenAPI JSON schema format.\n",
    "\n",
    "Define a tool that allows the Gemini model to select from the set of 3 functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a546a2d7-cffc-42a6-ba1a-e68f7b71a00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-pro\",\n",
    "    generation_config=GenerationConfig(temperature=0),\n",
    "    tools=[retail_tool],\n",
    ")\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1280d55d-e8db-4f6b-9df2-6757776a798d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retail_tool = Tool(\n",
    "    function_declarations=[\n",
    "        get_product_info,\n",
    "        get_store_location,\n",
    "        place_order,\n",
    "    ],\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dca6dc-cb18-4fa7-ba47-7bf4a3766e78",
   "metadata": {},
   "source": [
    "Now you can initialize the Gemini model with Function Calling in a multi-turn chat session.\n",
    "\n",
    "You can specify the tools kwarg when initializing the model to avoid having to send this kwarg with every subsequent request:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c24b0e-33f7-406c-b569-76d8e3b20545",
   "metadata": {},
   "source": [
    "Note: The temperature parameter controls the degree of randomness in this generation. Lower temperatures are good for functions that require deterministic parameter values, while higher temperatures are good for functions with parameters that accept more diverse or creative parameter values. A temperature of 0 is deterministic. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.\n",
    "\n",
    "We're ready to chat! Let's start the conversation by asking if a certain product is in stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a29aaff-cd59-4165-99a5-d4309bb25975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"get_product_info\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"product_name\"\n",
       "      value {\n",
       "        string_value: \"Pixel 8 Pro\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Do you have the Pixel 8 Pro in stock?\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3099c-2bab-4ddb-a839-92b40c5c92c0",
   "metadata": {},
   "source": [
    "The response from the Gemini API consists of a structured data object that contains the name and parameters of the function that Gemini selected out of the available functions.\n",
    "\n",
    "Since this notebook focuses on the ability to extract function parameters and generate function calls, you'll use mock data to feed synthetic responses back to the Gemini model rather than sending a request to an API server (not to worry, we'll make an actual API call in a later example!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa0d711-f349-4622-9245-6a466f9c13cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here you can use your preferred method to make an API request and get a response.\n",
    "# In this example, we'll use synthetic data to simulate a payload from an external API response.\n",
    "\n",
    "api_response = {\"sku\": \"GA04834-US\", \"in_stock\": \"yes\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebd32a-7abc-4b09-b871-2a65f81cb060",
   "metadata": {},
   "source": [
    "In reality, you would execute function calls against an external system or database using your desired client library or REST API.\n",
    "\n",
    "Now, you can pass the response from the (mock) API request and generate a response for the end user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7b5ec7f-4089-47a0-a373-ff6552d669bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, the Pixel 8 Pro is in stock. \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"get_product_info\",\n",
    "        response={\n",
    "            \"content\": api_response,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f4b21-7a2b-41a5-9031-00bb3f492408",
   "metadata": {},
   "source": [
    "Next, the user might ask where they can buy a different phone from a nearby store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa10cdd6-bb93-486e-9de2-2ff5a378749b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"get_product_info\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"product_name\"\n",
       "      value {\n",
       "        string_value: \"Pixel 8\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What about the Pixel 8? Is there a store in\n",
    "Mountain View, CA that I can visit to try one out?\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046267e-169e-455b-b2a3-4ad7fa290a46",
   "metadata": {},
   "source": [
    "Again, you get a response with structured data, and the Gemini model selected the get_product_info function. This happened since the user asked about the \"Pixel 8\" phone this time rather than the \"Pixel 8 Pro\" phone.\n",
    "\n",
    "Now you can build another synthetic payload that would come from an external API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "905ade92-a3ac-4b3b-9121-8580559d76a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here you can use your preferred method to make an API request and get a response.\n",
    "# In this example, we'll use synthetic data to simulate a payload from an external API response.\n",
    "\n",
    "api_response = {\"sku\": \"GA08475-US\", \"in_stock\": \"yes\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee141ea5-f72d-4c58-8a70-d58e0225381c",
   "metadata": {},
   "source": [
    "Again, you can pass the response from the (mock) API request back to the Gemini model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f190772-4847-4688-ba09-4718c32f3f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"get_store_location\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"location\"\n",
       "      value {\n",
       "        string_value: \"Mountain View, CA\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"get_product_info\",\n",
    "        response={\n",
    "            \"content\": api_response,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0fc0d-419a-42b0-b38b-c9020e01a0a4",
   "metadata": {},
   "source": [
    "Wait a minute! Why did the Gemini API respond with a second function call to get_store_location this time rather than a natural language summary? Look closely at the prompt that you used in this conversation turn a few cells up, and you'll notice that the user asked about a product -and- the location of a store.\n",
    "\n",
    "In cases like this when two or more functions are defined (or when the model predicts multiple function calls to the same function), the Gemini model might sometimes return back-to-back or parallel function call responses within a single conversation turn.\n",
    "\n",
    "This is expected behavior since the Gemini model predicts which functions it should call at runtime, what order it should call dependent functions in, and which function calls can be parallelized, so that the model can gather enough information to generate a natural language response.\n",
    "\n",
    "Not to worry! You can repeat the same steps as before and build another synthetic payload that would come from an external API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a3d2e11-3632-483a-8f30-722d2d72b5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here you can use your preferred method to make an API request and get a response.\n",
    "# In this example, we'll use synthetic data to simulate a payload from an external API response.\n",
    "\n",
    "api_response = {\"store\": \"2000 N Shoreline Blvd, Mountain View, CA 94043, US\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad37d0-3c9a-47dd-9296-ed5e9e7d8478",
   "metadata": {},
   "source": [
    "And you can pass the response from the (mock) API request back to the Gemini model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ebac6f1-d41a-477b-ab4b-97fd72ab803a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, the Pixel 8 is in stock. You can visit the store at 2000 N Shoreline Blvd, Mountain View, CA 94043, US to try one out. \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"get_store_location\",\n",
    "        response={\n",
    "            \"content\": api_response,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "response.text\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a0548-eb57-4607-923c-77513462483b",
   "metadata": {},
   "source": [
    "Nice work!\n",
    "\n",
    "Within a single conversation turn, the Gemini model requested 2 function calls in a row before returning a natural language summary. In reality, you might follow this pattern if you need to make an API call to an inventory management system, and another call to a store location database, customer management system, or document repository.\n",
    "\n",
    "Finally, the user might ask to order a phone and have it shipped to their address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa79b78d-550f-4d66-91aa-b925833b055c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"place_order\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"address\"\n",
       "      value {\n",
       "        string_value: \"1155 Borregas Ave, Sunnyvale, CA 94089\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"product\"\n",
       "      value {\n",
       "        string_value: \"Pixel 8 Pro\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I'd like to order a Pixel 8 Pro and have it shipped to 1155 Borregas Ave, Sunnyvale, CA 94089.\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6506c-0440-4975-a746-9965c30640d6",
   "metadata": {},
   "source": [
    "Perfect! The Gemini model extracted the user's selected product and their address. Now you can call an API to place the order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9765103b-d5f6-4a39-a713-bcd1c54e4163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is where you would make an API request to return the status of their order.\n",
    "# Use synthetic data to simulate a response payload from an external API.\n",
    "\n",
    "api_response = {\n",
    "    \"payment_status\": \"paid\",\n",
    "    \"order_number\": 12345,\n",
    "    \"est_arrival\": \"2 days\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e079d03c-59da-4a8f-b164-66847b894339",
   "metadata": {},
   "source": [
    "And send the payload from the external API call so that the Gemini API returns a natural language summary to the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97265907-7906-4521-a7f0-d1c3582c00b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your order has been placed and will arrive within 2 days. Your order number is 12345. \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"place_order\",\n",
    "        response={\n",
    "            \"content\": api_response,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e269e-dd5e-4200-8274-4d46c1d5dfd4",
   "metadata": {},
   "source": [
    "And you're done!\n",
    "\n",
    "You were able to have a multi-turn conversation with the Gemini model using function calls, handling payloads, and generating natural language summaries that incorporated the information from the external systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e344d0-7ebe-4429-aa69-ae7f90e64a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
